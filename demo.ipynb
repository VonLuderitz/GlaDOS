{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo the Phonemizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glados.TTS import phonemizer\n",
    "import glados.utils.spoken_text_converter as stc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = phonemizer.Phonemizer()\n",
    "glados_stc = stc.SpokenTextConverter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "input = \"Hello CPU, its 3:15 am! if you'll excuse me I'm GLaDOS, not GLadys.\"\n",
    "phonemes = p.convert_to_phonemes(input)\n",
    "print(phonemes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo the Text-to-Speech module\n",
    "### GLaDOS Voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "\n",
    "from glados.TTS import tts_glados as tts\n",
    "import glados.utils.spoken_text_converter as stc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "glados_tts = tts.Synthesizer()\n",
    "glados_stc = stc.SpokenTextConverter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "input = \"Hello, this is Glados, your fiendish assistant. Please upgrade your GPU!\"\n",
    "\n",
    "# Convert the text to intermediate representation that the TTS model can better pronounce\n",
    "intermediate = glados_stc.text_to_spoken(input)\n",
    "print(intermediate)\n",
    "\n",
    "# Generate the audio to from the text\n",
    "audio = glados_tts.generate_speech_audio(intermediate)\n",
    "\n",
    "# Play the audio\n",
    "sd.play(audio, glados_tts.sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the audio to a file\n",
    "import soundfile as sf\n",
    "\n",
    "sf.write(\"output.wav\", audio, glados_tts.sample_rate, format=\"WAV\", subtype=\"PCM_16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kokoko Voice\n",
    "\n",
    "Select from:\n",
    " - Female\n",
    "   - **US** - af_alloy, af_aoede, af_bella, af_jessica, af_kore, af_nicole, af_nova, af_river, af_sarah, af_sky\n",
    "   - **British** - bf_alice, bf_emma, bf_isabella, bf_lily\n",
    " - Male\n",
    "   - **US** - am_adam, am_echo, am_eric, am_fenrir, am_liam, am_michael, am_onyx, am_puck\n",
    "   - **British** - bm_daniel, bm_fable, bm_george, bm_lewis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "\n",
    "from glados.TTS import tts_kokoro as ktts\n",
    "import glados.utils.spoken_text_converter as stc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "kokoro_tts = ktts.Synthesizer(model_path=\"./models/TTS/kokoro-v1.0.fp16.onnx\")\n",
    "kokoro_stc = stc.SpokenTextConverter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "voice = \"af_bella\"\n",
    "input = \"Hello, this is Glados, your fiendish assistant. Please upgrade your GPU!\"\n",
    "\n",
    "# Convert the text to intermediate representation that the TTS model can better pronounce\n",
    "intermediate = kokoro_stc.text_to_spoken(input)\n",
    "print(intermediate)\n",
    "\n",
    "# Generate the audio to from the text\n",
    "audio = kokoro_tts.generate_speech_audio(intermediate, voice=voice)\n",
    "\n",
    "# Play the audio\n",
    "sd.play(audio, kokoro_tts.sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo the Automatic Speech Recogntion system\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glados.ASR import asr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriber = asr.AudioTranscriber()\n",
    "audio_path = \"data/0.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "transcription = transcriber.transcribe_file(audio_path)\n",
    "print(f\"Transcription: {transcription}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo the Vision System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from glados.Vision import ModelLoader, PromptProcessor, TextGenerator\n",
    "from glados.Vision.image_preprocessor import ImagePreprocessor\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"data/glados.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_config {'do_convert_rgb': True, 'do_image_splitting': True, 'do_normalize': True, 'do_pad': True, 'do_rescale': True, 'do_resize': True, 'image_mean': [0.5, 0.5, 0.5], 'image_processor_type': 'Idefics3ImageProcessor', 'image_std': [0.5, 0.5, 0.5], 'max_image_size': {'longest_edge': 512}, 'processor_class': 'Idefics3Processor', 'resample': 1, 'rescale_factor': 0.00392156862745098, 'size': {'longest_edge': 2048}}\n",
      "{'do_convert_rgb': True, 'do_image_splitting': True, 'do_normalize': True, 'do_pad': True, 'do_rescale': True, 'do_resize': True, 'image_mean': [0.5, 0.5, 0.5], 'image_processor_type': 'Idefics3ImageProcessor', 'image_std': [0.5, 0.5, 0.5], 'max_image_size': {'longest_edge': 512}, 'processor_class': 'Idefics3Processor', 'resample': 1, 'rescale_factor': 0.00392156862745098, 'size': {'longest_edge': 2048}}\n",
      "image [[[[ 60  94 104]\n",
      "   [ 60  94 104]\n",
      "   [ 60  94 104]\n",
      "   ...\n",
      "   [ 42  69  80]\n",
      "   [ 42  69  80]\n",
      "   [ 43  70  81]]\n",
      "\n",
      "  [[ 60  94 104]\n",
      "   [ 60  94 104]\n",
      "   [ 60  94 104]\n",
      "   ...\n",
      "   [ 42  69  80]\n",
      "   [ 42  69  80]\n",
      "   [ 42  69  80]]\n",
      "\n",
      "  [[ 60  94 104]\n",
      "   [ 60  94 104]\n",
      "   [ 60  94 104]\n",
      "   ...\n",
      "   [ 41  68  79]\n",
      "   [ 42  69  80]\n",
      "   [ 42  69  80]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 45  74  82]\n",
      "   [ 45  74  82]\n",
      "   [ 45  74  82]\n",
      "   ...\n",
      "   [ 52  84  95]\n",
      "   [ 52  84  97]\n",
      "   [ 52  84  97]]\n",
      "\n",
      "  [[ 45  74  82]\n",
      "   [ 45  74  82]\n",
      "   [ 45  74  82]\n",
      "   ...\n",
      "   [ 52  84  95]\n",
      "   [ 52  84  97]\n",
      "   [ 52  84  97]]\n",
      "\n",
      "  [[ 45  74  82]\n",
      "   [ 45  74  82]\n",
      "   [ 45  74  82]\n",
      "   ...\n",
      "   [ 52  84  95]\n",
      "   [ 52  84  97]\n",
      "   [ 52  84  97]]]]\n",
      "(1, 1080, 1920, 3)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot broadcast source array for assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprint_stream\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(text, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprint_stream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFinal response: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/GLaDOS/src/glados/Vision/text_generator.py:48\u001b[0m, in \u001b[0;36mTextGenerator.generate\u001b[0;34m(self, messages, image, max_new_tokens, callback)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate\u001b[39m(\u001b[38;5;28mself\u001b[39m, \n\u001b[1;32m     42\u001b[0m             messages: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m], \n\u001b[1;32m     43\u001b[0m             image: Image\u001b[38;5;241m.\u001b[39mImage, \n\u001b[1;32m     44\u001b[0m             max_new_tokens: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1024\u001b[39m,\n\u001b[1;32m     45\u001b[0m             callback: Callable[[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m     46\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate text response from input messages and image\"\"\"\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     52\u001b[0m     past_key_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_loader\u001b[38;5;241m.\u001b[39mcreate_past_key_values(batch_size)\n",
      "File \u001b[0;32m~/Documents/GLaDOS/src/glados/Vision/text_generator.py:20\u001b[0m, in \u001b[0;36mTextGenerator._process_inputs\u001b[0;34m(self, messages, image)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Process text and image inputs\"\"\"\u001b[39;00m\n\u001b[1;32m     19\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt_processor\u001b[38;5;241m.\u001b[39mapply_chat_template(messages, add_generation_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt_processor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GLaDOS/src/glados/Vision/prompt_processor.py:267\u001b[0m, in \u001b[0;36mPromptProcessor.preprocess\u001b[0;34m(self, text, images)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m, image)\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28mprint\u001b[39m(image\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m--> 267\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_processor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    270\u001b[0m     {\n\u001b[1;32m    271\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    276\u001b[0m     },\n\u001b[1;32m    277\u001b[0m ]\n\u001b[1;32m    279\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_chat_template(messages, add_generation_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/GLaDOS/src/glados/Vision/image_preprocessor.py:176\u001b[0m, in \u001b[0;36mImagePreprocessor.preprocess_image\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    173\u001b[0m new_width \u001b[38;5;241m=\u001b[39m new_width \u001b[38;5;241m+\u001b[39m (new_width \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (new_height, new_width) \u001b[38;5;241m!=\u001b[39m image\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m]:\n\u001b[0;32m--> 176\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mnumba_resize_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_height\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_width\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_image_splitting:\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;66;03m# Resize to multiple of patch size\u001b[39;00m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m new_height \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_size \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m new_width \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_size \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/GLaDOS/.venv/lib/python3.12/site-packages/numba/core/serialize.py:46\u001b[0m, in \u001b[0;36m_numba_unpickle\u001b[0;34m(address, bytedata, hashed)\u001b[0m\n\u001b[1;32m     44\u001b[0m key \u001b[38;5;241m=\u001b[39m (address, hashed)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 46\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43m_unpickled_memo\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m     _unpickled_memo[key] \u001b[38;5;241m=\u001b[39m obj \u001b[38;5;241m=\u001b[39m cloudpickle\u001b[38;5;241m.\u001b[39mloads(bytedata)\n",
      "\u001b[0;31mValueError\u001b[0m: cannot broadcast source array for assignment"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize components\n",
    "model_dir = Path(\"./models/Vision\")\n",
    "\n",
    "processor = PromptProcessor()\n",
    "model_loader = ModelLoader(model_dir=model_dir)\n",
    "generator = TextGenerator(model_loader, processor)\n",
    "\n",
    "# Prepare inputs\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": \"Can you describe this image?\"}\n",
    "        ]\n",
    "    },\n",
    "]\n",
    "image = Image.open(\"data/glados.jpeg\")\n",
    "\n",
    "# Generate with optional streaming\n",
    "def print_stream(text: str):\n",
    "    print(text, end='')\n",
    "\n",
    "response = generator.generate(messages, np.asarray([image]), callback=print_stream)\n",
    "print(f\"\\nFinal response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_config {'do_convert_rgb': True, 'do_image_splitting': True, 'do_normalize': True, 'do_pad': True, 'do_rescale': True, 'do_resize': True, 'image_mean': [0.5, 0.5, 0.5], 'image_processor_type': 'Idefics3ImageProcessor', 'image_std': [0.5, 0.5, 0.5], 'max_image_size': {'longest_edge': 512}, 'processor_class': 'Idefics3Processor', 'resample': 1, 'rescale_factor': 0.00392156862745098, 'size': {'longest_edge': 2048}}\n",
      "{'do_convert_rgb': True, 'do_image_splitting': True, 'do_normalize': True, 'do_pad': True, 'do_rescale': True, 'do_resize': True, 'image_mean': [0.5, 0.5, 0.5], 'image_processor_type': 'Idefics3ImageProcessor', 'image_std': [0.5, 0.5, 0.5], 'max_image_size': {'longest_edge': 512}, 'processor_class': 'Idefics3Processor', 'resample': 1, 'rescale_factor': 0.00392156862745098, 'size': {'longest_edge': 2048}}\n",
      "image [[[ 60  94 104]\n",
      "  [ 60  94 104]\n",
      "  [ 60  94 104]\n",
      "  ...\n",
      "  [ 42  69  80]\n",
      "  [ 42  69  80]\n",
      "  [ 43  70  81]]\n",
      "\n",
      " [[ 60  94 104]\n",
      "  [ 60  94 104]\n",
      "  [ 60  94 104]\n",
      "  ...\n",
      "  [ 42  69  80]\n",
      "  [ 42  69  80]\n",
      "  [ 42  69  80]]\n",
      "\n",
      " [[ 60  94 104]\n",
      "  [ 60  94 104]\n",
      "  [ 60  94 104]\n",
      "  ...\n",
      "  [ 41  68  79]\n",
      "  [ 42  69  80]\n",
      "  [ 42  69  80]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 45  74  82]\n",
      "  [ 45  74  82]\n",
      "  [ 45  74  82]\n",
      "  ...\n",
      "  [ 52  84  95]\n",
      "  [ 52  84  97]\n",
      "  [ 52  84  97]]\n",
      "\n",
      " [[ 45  74  82]\n",
      "  [ 45  74  82]\n",
      "  [ 45  74  82]\n",
      "  ...\n",
      "  [ 52  84  95]\n",
      "  [ 52  84  97]\n",
      "  [ 52  84  97]]\n",
      "\n",
      " [[ 45  74  82]\n",
      "  [ 45  74  82]\n",
      "  [ 45  74  82]\n",
      "  ...\n",
      "  [ 52  84  95]\n",
      "  [ 52  84  97]\n",
      "  [ 52  84  97]]]\n",
      "(1080, 1920, 3)\n",
      "here\n"
     ]
    }
   ],
   "source": [
    "processor = PromptProcessor()\n",
    "\n",
    "image = Image.open(\"data/glados.jpeg\")\n",
    "x = processor.preprocess(\"Can you describe this image?\", [np.asarray(image)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x['pixel_values']) == N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel_values (1, 13, 3, 512, 512) float32\n",
      "pixel_attention_mask (1, 1, 512, 512) bool\n",
      "input_ids (1, 872) int64\n",
      "attention_mask (1, 872) int64\n"
     ]
    }
   ],
   "source": [
    "for key in x:\n",
    "    print(key, x[key].shape, x[key].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glados.Vision.image_preprocessor import ImagePreprocessor\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'do_convert_rgb': True, 'do_image_splitting': True, 'do_normalize': True, 'do_pad': True, 'do_rescale': True, 'do_resize': True, 'image_mean': [0.5, 0.5, 0.5], 'image_processor_type': 'Idefics3ImageProcessor', 'image_std': [0.5, 0.5, 0.5], 'max_image_size': {'longest_edge': 512}, 'processor_class': 'Idefics3Processor', 'resample': 1, 'rescale_factor': 0.00392156862745098, 'size': {'longest_edge': 2048}}\n",
      "{'do_convert_rgb': True, 'do_image_splitting': True, 'do_normalize': True, 'do_pad': True, 'do_rescale': True, 'do_resize': True, 'image_mean': [0.5, 0.5, 0.5], 'image_processor_type': 'Idefics3ImageProcessor', 'image_std': [0.5, 0.5, 0.5], 'max_image_size': {'longest_edge': 512}, 'processor_class': 'Idefics3Processor', 'resample': 1, 'rescale_factor': 0.00392156862745098, 'size': {'longest_edge': 2048}}\n"
     ]
    }
   ],
   "source": [
    "config = json.load(open(\"models/Vision/preprocessor_config.json\"))\n",
    "print(config)\n",
    "imp = ImagePreprocessor(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n"
     ]
    }
   ],
   "source": [
    "x = imp.preprocess_image(np.asarray(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1080, 1920, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asanyarray(image).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 13, 3, 512, 512)\n",
      "(1, 1, 512, 512)\n"
     ]
    }
   ],
   "source": [
    "print(x['pixel_values'].shape)\n",
    "print(x['pixel_attention_mask'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'do_convert_rgb': True, 'do_image_splitting': True, 'do_normalize': True, 'do_pad': True, 'do_rescale': True, 'do_resize': True, 'image_mean': [0.5, 0.5, 0.5], 'image_processor_type': 'Idefics3ImageProcessor', 'image_std': [0.5, 0.5, 0.5], 'max_image_size': {'longest_edge': 512}, 'processor_class': 'Idefics3Processor', 'resample': 1, 'rescale_factor': 0.00392156862745098, 'size': {'longest_edge': 2048}}\n",
    "{'do_convert_rgb': True, 'do_image_splitting': True, 'do_normalize': True, 'do_pad': True, 'do_rescale': True, 'do_resize': True, 'image_mean': [0.5, 0.5, 0.5], 'image_processor_type': 'Idefics3ImageProcessor', 'image_std': [0.5, 0.5, 0.5], 'max_image_size': {'longest_edge': 512}, 'processor_class': 'Idefics3Processor', 'resample': 1, 'rescale_factor': 0.00392156862745098, 'size': {'longest_edge': 2048}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = x['pixel_values'][0, 12].transpose(1, 2, 0)\n",
    "\n",
    "# normalise image\n",
    "img = (img - img.min()) / (img.max() - img.min())\n",
    "\n",
    "\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = x['pixel_values'][0, 2].transpose(1, 2, 0)\n",
    "\n",
    "# normalise image\n",
    "img = (img - img.min()) / (img.max() - img.min())\n",
    "\n",
    "\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x['pixel_values'][11,:,:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.ones(x['pixel_values'].shape[0,1,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
